---
title: "Parallel Dithering: Is a speedup reachable ?"
author: Quentin Guilloteau
classoption: twocolumn
header-includes:
    - \usepackage{graphicx}
    - \usepackage{tikz}
output:
    pdf_document:
        keep_tex: true
        number_sections: true
    html_document:
        keep_tex: true
        number_sections: true
---

# Introduction

## Dithering

Dithering is the action to convert a grey scale image into an image composed only of black and white pixels.


\begin{figure}[h]
    \center
    \includegraphics[width=170px]{Images/charlie.png}
    \caption{Grey Scale Image}
\end{figure}

\begin{figure}[h]
    \center
    \includegraphics[width=170px]{Images/charlie_d.png}
    \caption{Dithered Image}
\end{figure}

## Floyd-Steinberg Dithering

There are many different possible ditherings.
They all use the same principles, only some numerical constants change.
In this paper, we will only focus on the __Floyd-Steinberg Dithering__.
It uses error diffusion.

The error diffusion matrix is the following:

$$
\displaystyle \left[ \begin{matrix}
    0 & 0 & 0 \\
    0 & 0 & \frac{7}{16} \\
    \frac{3}{16} & \frac{5}{16} & \frac{1}{16} \\
    \end{matrix}
    \right]
$$

\begin{figure}
\center
\begin{tikzpicture}
    \draw (0,0) grid (3,2);
    \draw [->] (1.5,1.5) -- (2.25,1.5);
    \draw [->] (1.5,1.5) -- (2.25,0.75);
    \draw [->] (1.5,1.5) -- (1.5,0.75);
    \draw [->] (1.5,1.5) -- (0.75,0.75);
    \draw (2.5, 1.5) node {$\frac{7}{16}$};
    \draw (2.5, 0.5) node {$\frac{1}{16}$};
    \draw (1.5, 0.5) node {$\frac{5}{16}$};
    \draw (0.5, 0.5) node {$\frac{3}{16}$};
\end{tikzpicture}
\caption{Error Diffusion for the Floyd-Steinberg Dithering}
\end{figure}

Figure \ref{fig:local_deps} shows another way to look at it, by considering the dependencies for a single pixel.

\begin{figure}
\center
\begin{tikzpicture}
    \draw (0,0) grid (3,2);
    \draw [->] (1.5,1.25) -- (1.5,0.75);
    \draw [->] (2.25,1.25) -- (1.75,0.75);
    \draw [->] (0.75,1.25) -- (1.25,0.75);
    \draw [->] (0.75,0.5) -- (1.25,0.5);
    \draw (2.5, 1.5) node {$\frac{3}{16}$};
    \draw (0.5, 1.5) node {$\frac{1}{16}$};
    \draw (1.5, 1.5) node {$\frac{5}{16}$};
    \draw (0.5, 0.5) node {$\frac{7}{16}$};
\end{tikzpicture}
\caption{Local Dependencies of the Floyd-Steinberg Dithering}
\label{fig:local_deps}
\end{figure}

## Pseudo-Code

We can write the pseudo code of the Floyd-Steinberg Dithering:

```C
for (y = 0; y < rows; y++) {
  for (x = 0; x < cols; x++) {
    // Computation of the error
    int old_value = pixels[y * cols + x];
    int new_value = (current_value < 127) ?
                    0 : 255;
    int error = old_value - new_value;
    pixels[y * cols + x] = new_value;

    // Error Propagation
    pixels[(y + 0) * cols + (x + 1)]
        += error * 7 / 16;
    pixels[(y + 1) * cols + (x + 1)]
        += error * 1 / 16;
    pixels[(y + 1) * cols + (x + 0)]
        += error * 5 / 16;
    pixels[(y + 1) * cols + (x - 1)]
        += error * 3 / 16;

  }
}
```

As we can see, this algorithm is *highly sequential*.
We have to start from the top left of the image and work ourselves to the right until we reach the end of the line.
Then we start again from the next line.


# Parallel Dithering

The main idea of the paralle dithering, is that the progression looks more like a triangle than a rectangle:


\begin{figure}
    \center
    \begin{tikzpicture}
        \draw (0,0) grid (5,4);
        \draw (0.5, 3.5) node {1};
        \draw [gray] (1.5, 3.5) node {2};
        \draw [red] (0.5, 2.5) node {3};
        \draw [red] (2.5, 3.5) node {3};

        \draw [orange] (3.5, 3.5) node {4};
        \draw [orange] (1.5, 2.5) node {4};

        \draw [blue] (0.5, 1.5) node {5};
        \draw [blue] (2.5, 2.5) node {5};
        \draw [blue] (4.5, 3.5) node {5};
    \end{tikzpicture}

\caption{Potential Parallel Execution}
\label{fig:ppe}
\end{figure}

Figure \ref{fig:ppe}, shows that there are indeed some possible parallism in the dithering process.
The numbers correspond to the order of the execution.
When some pixels have the same number, it means that they can be processed in parallel.

We also see that each line needs to be 2 pixels ahead of the line below.


## Alternate processes

### Presentation

The first idea is to alternate processes and giving them one line at the time to work with.

\begin{figure}
    \center
    \begin{tikzpicture}
        \draw (0,0) grid (5,4);
        \draw [blue,thick] (0,3.02) rectangle (5, 3.98);
        \draw [red,thick] (0,2.02) rectangle (5, 2.98);
        \draw [blue,thick] (0,1.02) rectangle (5, 1.98);
        \draw [red,thick] (0,0.02) rectangle (5, 0.98);

        \draw [blue] (2.5, 1.5) node {Process 0};
        \draw [blue] (2.5, 3.5) node {Process 0};
        \draw [red] (2.5, 0.5) node {Process 1};
        \draw [red] (2.5, 2.5) node {Process 1};
    \end{tikzpicture}

\caption{Data Distribution per Process}
\label{fig:alternate_processes}
\end{figure}

Figure \ref{fig:alternate_processes} gives an example of the distribution of data between 2 processes.

Process 0 will start working on the first pixel of its line.
As we saw in figure \ref{fig:ppe}, a process must be done processing pixel $n$ of its line for the next process to be able to process the pixel $n - 2$ of its own line.

### Implemenation Details

### Performance Analysis
\label{sec:alternate}

Let $p$ be the number of processors.
Let us consider a square image of size $n$.
Let $L$ be the latency of the network and $B$ its bandwidth.
Let $w$ be the processing time of a pixel.

It takes $2 \times ( p - 1)$ steps before processor $P_{p - 1}$ can start processing the first pixel of its first line.

Each processor has $\frac{n^2}{p}$ pixels.

Therefore there are in total $2\times ( p - 1) + \frac{n^2}{p}$ steps.

We thus have:

\begin{equation}
T(n, p) = \left( 2\times ( p - 1) + \frac{n^2}{p} \right) \left( w + L + \frac{1}{B}\right)
\end{equation}

The sequential time is: $w\times n^2$.

We can thus compute the speedup:

$$
\displaystyle S(n, p) = \frac{w \times n^2}{\left( 2\times ( p - 1) + \frac{n^2}{p} \right) \left( w + L + \frac{1}{B}\right)}
$$

The limit speedup is thus:

\begin{equation}
\displaystyle \lim_{n \rightarrow \infty} S(n, p) = \frac{w \times p}{w + L + \frac{1}{B}}
\end{equation}

The efficiency of this algorithm is:

\begin{equation}
\label{eq:eff1}
\displaystyle Eff(n, p)  = \lim_{n \rightarrow \infty} \frac{S(n, p)}{p} = \frac{1}{1 + \frac{L}{w} + \frac{1}{B \times w}}
\end{equation}

The efficiency of this algorithm (equation \ref{eq:eff1}) is less than one.
It thus means that it is not very efficient.

## Reducing the Granularity

### Presentation

Sending the error every time a process process a pixel introduce too much loss due to the latency.
We will try to reduce the granularity of the algorithm by grouping pixels per block.
We will then also send the errors per block.

TODO IMAGE

TODO Compute the min block size to be ok

### Upper bound for $k$

Let $w$ be the width of the image and $p$ be the number of processes.

In order to not have any idle time by the processes, we would like the process $P_{p-1}$ to have at least finished processing its first 2 blocks when process $P_{0}$ is done processing its line.

Otherwise, process $P_{0}$ would have to wait for process $P_{p-1}$ to send the error of the first block.

Let $k_{hi}$ be the lower bound of $k$ such that there is no busy waiting by the processes.

There are $\displaystyle \frac{w}{k}$ blocks to process on one line.

Once the first line will be done by process $P_{0}$, process $P_{p-1}$ would have processed $\frac{w}{k} - 2\times (p - 1)$.

We want the last process to have processed at least 2 blocks (so it can send the error of the first block to process $P_{0}$).

Thus, 

\begin{equation}
\frac{w}{k} - 2\times (p - 1) \geq 2 \implies k \leq \frac{w}{2 \times p} = k_{hi}
\end{equation}

### Implementation Details

### Performance Analysis

We pretty much have the same logic than in section \ref{sec:alternate}, with blocks instead of pixels.

We note $k$ the size of a block.
So a block contains $k$ pixels.

\begin{equation}
T(n, p) = \left( 2\times ( p - 1) + \frac{n^2}{p \times k} \right) \left( w \times k + L + \frac{k}{B}\right)
\end{equation}

We compute next the speedup of this new version:

$$
\displaystyle S(n, p) = \frac{w \times n^2}{\left( 2\times ( p - 1) + \frac{n^2}{p\times k} \right) \left( w \times k + L + \frac{k}{B}\right)}
$$

The limit speedup is thus:

\begin{equation}
\displaystyle \lim_{n \rightarrow \infty} S(n, p) = \frac{w \times p \times k}{w \times k + L + \frac{k}{B}}
\end{equation}

The efficiency of this algorithm is:

\begin{equation}
\label{eq:eff2}
\displaystyle Eff(n, p)  = \lim_{n \rightarrow \infty} \frac{S(n, p)}{p} = \frac{1}{1 + \frac{L}{w \times k} + \frac{1}{B\times w}}
\end{equation}

We see that we improve the efficiency of the algorithm by limiting the use of the network and reducing the global cost of the latency.

## Limiting the Impact of the Bandwidth

### Presentation

In order to reduce the impact of the bandwith, we must send less messages through the network.
In the previous section, we increased the size of the messages by sending pixels per block.
In this section, we will create blocks of lines.
Each process will have several blocks of lines.
Each block of line will contain $r$ lines of pixels.
Only the top and bottom lines of the block will require communications.
The remaining pixels will be processed sequentially.


TODO IMAGE

### Implementation Details

### Performance Anlysis

We pretty much have the same logic than in section \ref{sec:alternate}, with blocks instead of pixels.

\begin{equation}
T(n, p) = \left( 2\times ( p - 1) + \frac{n^2}{p \times k \times r} \right) \left( w \times k \times r + L + \frac{k}{B}\right)
\end{equation}

We compute next the speedup of this new version:

$$
\displaystyle S(n, p) = \frac{w \times n^2}{\left( 2\times ( p - 1) + \frac{n^2}{p\times k \times r} \right) \left( w \times k \times r + L + \frac{k}{B}\right)}
$$

The limit speedup is thus:

\begin{equation}
\displaystyle \lim_{n \rightarrow \infty} S(n, p) = \frac{w \times p \times k \times r}{w \times k \times r + L + \frac{k}{B}}
\end{equation}

The efficiency of this algorithm is:

\begin{equation}
\label{eq:eff3}
\displaystyle Eff(n, p)  = \lim_{n \rightarrow \infty} \frac{S(n, p)}{p} = \frac{1}{1 + \frac{L}{w \times k \times r} + \frac{1}{B\times w \times r}}
\end{equation}

Once again, we manage to improve the efficiency of the algorithm by limiting the number of messages send.
However, the efficiency if always less than 1.

# Performances

# Conclusion

