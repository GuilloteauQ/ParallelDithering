---
title: "Parallel Dithering: Is a speedup reachable ?"
author: Quentin Guilloteau
classoption: twocolumn
header-includes:
    - \usepackage{graphicx}
    - \usepackage{tikz}
    - \usetikzlibrary{decorations.pathreplacing,angles,quotes}
output:
    pdf_document:
        keep_tex: true
        number_sections: true
    html_document:
        keep_tex: true
        number_sections: true
---

# Introduction

## Dithering

Dithering is the action to convert a grey scale image into an image composed only of black and white pixels.


\begin{figure}[h]
    \center
    \includegraphics[width=170px]{Images/charlie.png}
    \caption{Grey Scale Image}
\end{figure}

\begin{figure}[h]
    \center
    \includegraphics[width=170px]{Images/charlie_d.png}
    \caption{Dithered Image}
\end{figure}

## Floyd-Steinberg Dithering

There are many different possible ditherings.
They all use the same principles, only some numerical constants change.
In this paper, we will only focus on the __Floyd-Steinberg Dithering__.
It uses error diffusion.

The error diffusion matrix is the following:

$$
\displaystyle \left[ \begin{matrix}
    0 & 0 & 0 \\
    0 & 0 & \frac{7}{16} \\
    \frac{3}{16} & \frac{5}{16} & \frac{1}{16} \\
    \end{matrix}
    \right]
$$

\begin{figure}
\label{fig:propagation}
\center
\begin{tikzpicture}
    \draw (0,0) grid (3,2);
    \draw [->] (1.5,1.5) -- (2.25,1.5);
    \draw [->] (1.5,1.5) -- (2.25,0.75);
    \draw [->] (1.5,1.5) -- (1.5,0.75);
    \draw [->] (1.5,1.5) -- (0.75,0.75);
    \draw (2.5, 1.5) node {$\frac{7}{16}$};
    \draw (2.5, 0.5) node {$\frac{1}{16}$};
    \draw (1.5, 0.5) node {$\frac{5}{16}$};
    \draw (0.5, 0.5) node {$\frac{3}{16}$};
\end{tikzpicture}
\caption{Error Diffusion for the Floyd-Steinberg Dithering}
\end{figure}

Figure \ref{fig:local_deps} shows another way to look at it, by considering the dependencies for a single pixel.

\begin{figure}
\center
\begin{tikzpicture}
    \draw (0,0) grid (3,2);
    \draw [->] (1.5,1.25) -- (1.5,0.75);
    \draw [->] (2.25,1.25) -- (1.75,0.75);
    \draw [->] (0.75,1.25) -- (1.25,0.75);
    \draw [->] (0.75,0.5) -- (1.25,0.5);
    \draw (2.5, 1.5) node {$\frac{3}{16}$};
    \draw (0.5, 1.5) node {$\frac{1}{16}$};
    \draw (1.5, 1.5) node {$\frac{5}{16}$};
    \draw (0.5, 0.5) node {$\frac{7}{16}$};
\end{tikzpicture}
\caption{Local Dependencies of the Floyd-Steinberg Dithering}
\label{fig:local_deps}
\end{figure}

## Pseudo-Code

We can write the pseudo code of the Floyd-Steinberg Dithering:

```C
for (y = 0; y < rows; y++) {
  for (x = 0; x < cols; x++) {
    // Computation of the error
    int old_value = pixels[y * cols + x];
    int new_value = (current_value < 127) ?
                    0 : 255;
    int error = old_value - new_value;
    pixels[y * cols + x] = new_value;

    // Error Propagation
    pixels[(y + 0) * cols + (x + 1)]
        += error * 7 / 16;
    pixels[(y + 1) * cols + (x + 1)]
        += error * 1 / 16;
    pixels[(y + 1) * cols + (x + 0)]
        += error * 5 / 16;
    pixels[(y + 1) * cols + (x - 1)]
        += error * 3 / 16;

  }
}
```

As we can see, this algorithm is _highly sequential_.
We have to start from the top left of the image and work ourselves to the right until we reach the end of the line.
Then we start again from the next line.


# Parallel Dithering

The main idea of the paralle dithering, is that the progression looks more like a triangle than a rectangle:


\begin{figure}
    \center
    \begin{tikzpicture}
        \draw (0,0) grid (5,4);
        \draw (0.5, 3.5) node {1};
        \draw [gray] (1.5, 3.5) node {2};
        \draw [red] (0.5, 2.5) node {3};
        \draw [red] (2.5, 3.5) node {3};

        \draw [orange] (3.5, 3.5) node {4};
        \draw [orange] (1.5, 2.5) node {4};

        \draw [blue] (0.5, 1.5) node {5};
        \draw [blue] (2.5, 2.5) node {5};
        \draw [blue] (4.5, 3.5) node {5};
    \end{tikzpicture}

\caption{Potential Parallel Execution}
\label{fig:ppe}
\end{figure}

Figure \ref{fig:ppe}, shows that there are indeed some possible parallism in the dithering process.
The numbers correspond to the order of the execution.
When some pixels have the same number, it means that they can be processed in parallel.

We also see that each line needs to be 2 pixels ahead of the line below.


## Alternate processes

### Presentation

The first idea is to alternate processes and giving them one line at the time to work with.

\begin{figure}
    \center
    \begin{tikzpicture}
        \draw (0,0) grid (5,4);
        \draw [blue,thick] (0,3.02) rectangle (5, 3.98);
        \draw [red,thick] (0,2.02) rectangle (5, 2.98);
        \draw [blue,thick] (0,1.02) rectangle (5, 1.98);
        \draw [red,thick] (0,0.02) rectangle (5, 0.98);

        \draw [blue] (2.5, 1.5) node {Process 0};
        \draw [blue] (2.5, 3.5) node {Process 0};
        \draw [red] (2.5, 0.5) node {Process 1};
        \draw [red] (2.5, 2.5) node {Process 1};
    \end{tikzpicture}

\caption{Data Distribution per Process}
\label{fig:alternate_processes}
\end{figure}

Figure \ref{fig:alternate_processes} gives an example of the distribution of data between 2 processes.

Process 0 will start working on the first pixel of its line.
As we saw in figure \ref{fig:ppe}, a process must be done processing pixel $n$ of its line for the next process to be able to process the pixel $n - 2$ of its own line.

### Implemenation Details

#### Data Structure

TODO

#### Distributing the lines to the processes

As one process has non adjacent lines, we have to define our own ``MPI_Vector_Type`` to properly send the correct lines to each process.

As we want process $P_{i}$ to have $\frac{n}{p}$ lines with $p - 1$ lines between each line and stating with the $i$th line.

```{C}
// Definition of the custom type
MPI_Type_vector(h / world_size,
                w, world_size * w,
                MPI_INT16_T, &PixelLine);
MPI_Type_commit(&PixelLine);
```

Unfortunatly, it is not possible to simply use the ``MPI_Scatter`` function to send the pixels to the processes.
Indeed, it will start the next ``PixelLine`` at the end of the previous one, however, we want it to start at the same position that the previous one with an offset of ``w``.

So we decided to simply call the ``MPI_Send`` function manually to scatter the lines among the processes.

#### Processing a line

Apart for the first line of the first process, every process has to receive the error from the above process to be able to process the pixels of its current line.

```{C}
MPI_Recv(&error_from_top, 1, MPI_INT16_T,
    (my_rank + world_size - 1) % world_size,
    0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
```

Once the error from has been received, we need to update the value of the current pixel.

```{C}
local_pixels[i + w * line_index]
        += error_from_top;
```

At this point, we can compute the new value for this pixel and its error to propagate.

```{C}
int16_t current_value =
    local_pixels[i + w * line_index];
int16_t new_value = (current_value < 127) ?
                    0 : 255;
local_pixels[i + w * line_index] = new_value;
int16_t error = current_value - new_value;
```

#### Propagating the error to the process below

As figure \ref{fig:propagation} shows, once its value updated, one pixel has to send its error to (at most) 3 pixels to the line below.
However, in order for the first error of the line to be send to the process below, we need to wait for the second pixel to be done processing as we want to send the error only once.

We decided to use a circular buffer of size 3 to manage this issue.

This buffer will store the cumulated errors to send to the process below until they are ready to be send.
We need a size of 3 because a pixel propagate its error to (at most) 3 pixels on the line below.

\begin{figure}
    \center
    \begin{tikzpicture}
        % Drawing the lines and buffers
        \draw (0,0) grid (4,1);
        \draw (0,2) grid (3,3);
        \draw (0,4) grid (4,5);

        % Drawing the lines
        \draw [->, dashed] (0.5,4.5) -- (0.5,2.75);
        \draw [->, dashed] (0.5,4.5) -- (1.25,2.75);
        \draw [black] (0.5, 4.75) node {1};
        \draw (0.5, 4.75) circle (0.15);

        \draw [->, dashed] (1.5,4.5) -- (0.75,2.75);
        \draw [->, dashed] (1.5,4.5) -- (1.5,2.75);
        \draw [->, dashed] (1.5,4.5) -- (2.25,2.75);
        \draw [black] (1.5, 4.75) node {2};
        \draw (1.5, 4.75) circle (0.15);

        \draw [->] (0.5,2.5) -- node {send} (0.5,0.5);
        \draw [black] (-0.1, 1.47) node {3};
        \draw (-0.1, 1.47) circle (0.15);

        % Legend
        \draw (-1, 4.5) node {line $i$};
        \draw (-1, 2.5) node {Buffer};
        \draw (-1, 0.5) node {line $i + 1$};

       \draw[decoration={brace,mirror,raise=5pt},decorate](-1.7,5) -- node[left=6pt] {$P_{k}$} (-1.7,2);
       \draw[decoration={brace,mirror,raise=5pt},decorate](-1.7,1) -- node[left=6pt] {$P_{k + 1}$} (-1.7,0);


    \end{tikzpicture}

\caption{Use of the cicular buffer}
\label{fig:circular buffer}
\end{figure}

Figure \ref{fig:circular_buffer} resumes the mechanism used with the buffer.

\begin{enumerate}
    \item The first pixel of line $i$ is processed on $P_{k}$. We add the proportions of the error to the corresponding cells of the buffer.
    \item The second pixel of line $i$ is processed on $P_{k}$. We add the propotions of the error to the corresponding cells of the buffer.
    \item The first cell of the buffer is ready to be send to $P_{k+1}$ (no more depencies). It contains the cumulated error for the first pixel of line $i+1$.
    \item Once send, we set the value of the sent cell back to 0.
\end{enumerate}


### Performance Analysis
\label{sec:alternate}

Let $p$ be the number of processors.
Let us consider a square image of size $n$.
Let $L$ be the latency of the network and $B$ its bandwidth.
Let $w$ be the processing time of a pixel.

It takes $2 \times ( p - 1)$ steps before processor $P_{p - 1}$ can start processing the first pixel of its first line.

Each processor has $\frac{n^2}{p}$ pixels.

Therefore there are in total $2\times ( p - 1) + \frac{n^2}{p}$ steps.

We thus have:

\begin{equation}
T(n, p) = \left( 2\times ( p - 1) + \frac{n^2}{p} \right) \times \left( w + L + \frac{1}{B}\right)
\end{equation}

The sequential time is: $w\times n^2$.

We can thus compute the speedup:

$$
\displaystyle S(n, p) = \frac{w \times n^2}{\left( 2\times ( p - 1) + \frac{n^2}{p} \right) \left( w + L + \frac{1}{B}\right)}
$$

The limit speedup is thus:

\begin{equation}
\displaystyle \lim_{n \rightarrow \infty} S(n, p) = \frac{w \times p}{w + L + \frac{1}{B}}
\end{equation}

The efficiency of this algorithm is:

\begin{equation}
\label{eq:eff1}
\displaystyle Eff(n, p)  = \lim_{n \rightarrow \infty} \frac{S(n, p)}{p} = \frac{1}{1 + \frac{L}{w} + \frac{1}{B \times w}}
\end{equation}

The efficiency of this algorithm (equation \ref{eq:eff1}) is less than one.
It thus means that it is not very efficient.

## Reducing the Granularity

### Presentation

Sending the error every time a process process a pixel introduce too much loss due to the latency.
We will try to reduce the granularity of the algorithm by grouping pixels per block.
We will then also send the errors per block.

TODO IMAGE

TODO Compute the min block size to be ok

### Upper bound for $k$

Let $w$ be the width of the image and $p$ be the number of processes.

In order to not have any idle time by the processes, we would like the process $P_{p-1}$ to have at least finished processing its first 2 blocks when process $P_{0}$ is done processing its line.

Otherwise, process $P_{0}$ would have to wait for process $P_{p-1}$ to send the error of the first block.

Let $k_{hi}$ be the lower bound of $k$ such that there is no busy waiting by the processes.

There are $\displaystyle \frac{w}{k}$ blocks to process on one line.

Once the first line will be done by process $P_{0}$, process $P_{p-1}$ would have processed $\frac{w}{k} - 2\times (p - 1)$.

We want the last process to have processed at least 2 blocks (so it can send the error of the first block to process $P_{0}$).

Thus,

\begin{equation}
\frac{w}{k} - 2\times (p - 1) \geq 2 \implies k \leq \frac{w}{2 \times p} = k_{hi}
\end{equation}

### Implementation Details

### Performance Analysis

We pretty much have the same logic than in section \ref{sec:alternate}, with blocks instead of pixels.

We note $k$ the size of a block.
So a block contains $k$ pixels.

\begin{equation}
T(n, p) = \left( 2\times ( p - 1) + \frac{n^2}{p \times k} \right) \left( w \times k + L + \frac{k}{B}\right)
\end{equation}

We compute next the speedup of this new version:

$$
\displaystyle S(n, p) = \frac{w \times n^2}{\left( 2\times ( p - 1) + \frac{n^2}{p\times k} \right) \times \left( w \times k + L + \frac{k}{B}\right)}
$$

The limit speedup is thus:

\begin{equation}
\displaystyle \lim_{n \rightarrow \infty} S(n, p) = \frac{w \times p \times k}{w \times k + L + \frac{k}{B}}
\end{equation}

The efficiency of this algorithm is:

\begin{equation}
\label{eq:eff2}
\displaystyle Eff(n, p)  = \lim_{n \rightarrow \infty} \frac{S(n, p)}{p} = \frac{1}{1 + \frac{L}{w \times k} + \frac{1}{B\times w}}
\end{equation}

We see that we improve the efficiency of the algorithm by limiting the use of the network and reducing the global cost of the latency.

## Limiting the Impact of the Bandwidth

### Presentation

In order to reduce the impact of the bandwith, we must send less messages through the network.
In the previous section, we increased the size of the messages by sending pixels per block.
In this section, we will create blocks of lines.
Each process will have several blocks of lines.
Each block of line will contain $r$ lines of pixels.
Only the top and bottom lines of the block will require communications.
The remaining pixels will be processed sequentially.


TODO IMAGE

### Implementation Details

### Performance Anlysis

We pretty much have the same logic than in section \ref{sec:alternate}, with blocks instead of pixels.

\begin{equation}
T(n, p) = \left( 2\times ( p - 1) + \frac{n^2}{p \times k \times r} \right) \times \left( w \times k \times r + L + \frac{k}{B}\right)
\end{equation}

We compute next the speedup of this new version:

$$
\displaystyle S(n, p) = \frac{w \times n^2}{\left( 2\times ( p - 1) + \frac{n^2}{p\times k \times r} \right) \left( w \times k \times r + L + \frac{k}{B}\right)}
$$

The limit speedup is thus:

\begin{equation}
\displaystyle \lim_{n \rightarrow \infty} S(n, p) = \frac{w \times p \times k \times r}{w \times k \times r + L + \frac{k}{B}}
\end{equation}

The efficiency of this algorithm is:

\begin{equation}
\label{eq:eff3}
\displaystyle Eff(n, p)  = \lim_{n \rightarrow \infty} \frac{S(n, p)}{p} = \frac{1}{1 + \frac{L}{w \times k \times r} + \frac{1}{B\times w \times r}}
\end{equation}

Once again, we manage to improve the efficiency of the algorithm by limiting the number of messages send.
However, the efficiency if always less than 1.

# Performances

# Conclusion

